{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Notebook\n",
    "This notebook will preprocess and leverage NLP models on the unstructured data to turn it into a usable feature space for modeling Tucker Carlson's body of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports cell\n",
    "\n",
    "#Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fox News host gives his take on pro-abortion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fox News host reflects on the left's respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fox News host gives his take on how Americans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fox News host gives his take on the Supreme C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fox News host gives his take on the real moti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0   Fox News host gives his take on pro-abortion ...\n",
       "1    Fox News host reflects on the left's respons...\n",
       "2   Fox News host gives his take on how Americans...\n",
       "3   Fox News host gives his take on the Supreme C...\n",
       "4   Fox News host gives his take on the real moti..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the tucker document data either as a CSV or a pickle\n",
    "#Read out from CSV\n",
    "tucker_docs = pd.read_csv('data/tucker_docs.csv', encoding='UTF8', header = None).T\n",
    "tucker_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stem or to lem? We will lem\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    #step 1: delete all caps words\n",
    "    t_d = re.sub(r'\\b[A-Z]+\\b', '', text)\n",
    "    \n",
    "    #step 2: tokenize\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenized_doc = nltk.regexp_tokenize(t_d, pattern)\n",
    "    \n",
    "    #step 3: lower all cases\n",
    "    low_tokenized_doc = [word.lower() for word in tokenized_doc]\n",
    "    \n",
    "    #step 4: stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stop_tokenized_doc = [word for word in low_tokenized_doc if word not in stopwords_list]\n",
    "    \n",
    "    #step 5: lem\n",
    "    tokens = [wnl.lemmatize(word) for word in stop_tokenized_doc]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of preprocessed Tucker Carlson episodes\n",
    "tucker_list = tucker_docs[0].tolist()\n",
    "new_list = []\n",
    "for each_doc in tucker_list:\n",
    "    new_list.append(preprocessing(each_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abaca</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abasement</th>\n",
       "      <th>abbott</th>\n",
       "      <th>...</th>\n",
       "      <th>zoomcall</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zoonotic</th>\n",
       "      <th>zot</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerbucks</th>\n",
       "      <th>zvfcgesbfiy</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron   ab  abaca  aback  abandon  abandoned  abandoning  abandonment  \\\n",
       "0    0.0  0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n",
       "1    0.0  0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n",
       "2    0.0  0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n",
       "3    0.0  0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n",
       "4    0.0  0.0    0.0    0.0      0.0        0.0         0.0          0.0   \n",
       "\n",
       "   abasement  abbott  ...  zoomcall  zoomed  zoonotic  zot  zucker  \\\n",
       "0        0.0     0.0  ...       0.0     0.0       0.0  0.0     0.0   \n",
       "1        0.0     0.0  ...       0.0     0.0       0.0  0.0     0.0   \n",
       "2        0.0     0.0  ...       0.0     0.0       0.0  0.0     0.0   \n",
       "3        0.0     0.0  ...       0.0     0.0       0.0  0.0     0.0   \n",
       "4        0.0     0.0  ...       0.0     0.0       0.0  0.0     0.0   \n",
       "\n",
       "   zuckerberg  zuckerbucks  zvfcgesbfiy  zweig   zz  \n",
       "0         0.0          0.0          0.0    0.0  0.0  \n",
       "1         0.0          0.0          0.0    0.0  0.0  \n",
       "2         0.0          0.0          0.0    0.0  0.0  \n",
       "3         0.0          0.0          0.0    0.0  0.0  \n",
       "4         0.0          0.0          0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 23434 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate IDF vectorizer to create vectorized array\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vect = vectorizer.fit_transform([' '.join(new_list[n]) for n in range(len(new_list))])\n",
    "td_idf_df = pd.DataFrame(vect.toarray(), columns = vectorizer.get_feature_names())\n",
    "td_idf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the NMF topic generation model\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components = 10)\n",
    "model.fit(td_idf_df)\n",
    "\n",
    "# to get H\n",
    "H = model.transform(td_idf_df) # transform document into topic vector representation\n",
    "\n",
    "# to get W \n",
    "W = model.components_ # word component weights for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['thing', 'year', 'see', 'new', 'state', 'would', 'yes', 'say', 'mean', 'want', 'get', 'thank', 'country', 'one', 'like', 'well', 'right', 'they', 'think', 'going', 'know', 'we', 'that', 're', 'people']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['that', 'they', 'american', 'military', 'going', 'biden', 'sanction', 'would', 'weapon', 'nuclear', 'know', 'united', 'invasion', 'energy', 'president', 'state', 'we', 're', 'vladimir', 'ukrainian', 'war', 'putin', 'russian', 'russia', 'ukraine']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['function', 'gain', 'boogeyman', 'really', 'lab', 'said', 'look', 'wuhan', 'people', 'celebrate', 'public', 'infection', 'know', 'science', 'virus', 'scientist', 'pandemic', 'immunity', 'research', 'anthony', 'email', 'dr', 'christmas', 'tony', 'fauci']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['mandate', 'crime', 'terrorism', 'biden', 'justice', 'violence', 'department', 'teach', 'education', 'terrorist', 'people', 'like', 'meeting', 're', 'they', 'domestic', 'teacher', 'merrick', 'kid', 'garland', 'mask', 'child', 'board', 'parent', 'school']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['hunter', 'state', 'vice', 'would', 'way', 'going', 'thing', 'administration', 'he', 'carlson', 'democratic', 'year', 'unity', 'that', 'like', 'american', 'white', 'one', 'kamala', 'country', 'people', 'harris', 'president', 'joe', 'biden']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['would', 'one', 'country', 'like', 'translator', 'son', 're', 'they', 'know', 'scheller', 'marine', 'biden', 'airport', 'state', 'refugee', 'pentagon', 'bagram', 'people', 'american', 'military', 'kabul', 'ghani', 'afghan', 'taliban', 'afghanistan']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['want', 'carlson', 'that', 'campaign', 'presidential', 'they', 'pelosi', 'election', 'people', 'would', 'elizabeth', 'bernie', 'candidate', 'like', 'sander', 'democratic', 'republican', 'voter', 'president', 'party', 'democrat', 'donald', 'impeachment', 'warren', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['clinic', 'supreme', 'say', 'harris', 'body', 'cervix', 'conversation', 'toobin', 'get', 'cat', 'like', 'steinem', 'transphobic', 'black', 'court', 'ban', 'roe', 'reproductive', 'people', 'hitler', 'men', 'baby', 'pregnant', 'woman', 'abortion']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['law', 'columbus', 'like', 'year', 'kenosha', 'shooting', 'mayor', 'black', 'officer', 'one', 'crime', 'white', 'gun', 'shot', 'capitol', 'violence', 'mob', 'kyle', 'lori', 'people', 'rittenhouse', 'city', 'lightfoot', 'police', 'chicago']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['lockdown', 'world', 'american', 'know', 'vaccination', 'new', 'wuhan', 'wear', 'one', 'risk', 'wearing', 'public', 'pandemic', 'government', 'science', 'health', 'vaccinated', 'chinese', 'china', 'virus', 'coronavirus', 'people', 'australia', 'mask', 'vaccine']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#List out the top 10 words for each topic\n",
    "\n",
    "for index,topic in enumerate(W):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-25:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#Visualize the 10 topics\n",
    "\n",
    "%%capture topic_word_plot\n",
    "def plot_top_words(W, feature_names, n_top_words, title, n_topics):\n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(15, 12), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(W):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "n_top_words = 20\n",
    "tfidf_feature_names = vectorizer.get_feature_names()\n",
    "plot_top_words(W, tfidf_feature_names, n_top_words, \"Topics in NMF model\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_word_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bf4caab80706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopic_word_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_word_plot' is not defined"
     ]
    }
   ],
   "source": [
    "topic_word_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually re-assign topics\n",
    "topic = {0: 'American Politics', 1: 'War in Ukraine', 2: 'Covid-19', 3: 'School', 4: 'Biden Administration', \n",
    "         5: 'War in Afghanistan', 6: 'Trump', 7: 'Abortion', 8: 'Kyle Rittenhouse Shooting', \n",
    "         9: 'Covid-19 Vaccine' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('data/unique_tweets_list.csv').drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function library\n",
    "import ast\n",
    "\n",
    "def string_to_list(s):\n",
    "    try:\n",
    "        l = ast.literal_eval(s)\n",
    "    except:\n",
    "        l = None\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweets(tweet_row):\n",
    "    tweet_list = []\n",
    "    for n in range(len(tweet_row)):\n",
    "        tweet_list.append(preprocessing(tweet_row[n]))\n",
    "\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweet_list:\n",
    "        if len(tweet) > 20:\n",
    "            cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess each user's tweet history\n",
    "tweet_df['tweet_history'] = tweet_df['tweet_history'].apply(lambda x: string_to_list(x))\n",
    "tweet_df = tweet_df.dropna()\n",
    "tweet_df['cleaned_tweets'] = tweet_df['tweet_history'].apply(lambda x: prepare_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_history</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Legislative genius\" Nancy @TeamPelosi needs ...</td>\n",
       "      <td>[[imagine, female, journos, stay, twitter, mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[withoutrunes: wack-ashimself: withoutrunes: w...</td>\n",
       "      <td>[[withoutrunes, wack, ashimself, withoutrunes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Was just sent this after complaining about de...</td>\n",
       "      <td>[[hahahahaha, omg, first, javascript, course, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Over turn Roe V Wade fine but be prepared to ...</td>\n",
       "      <td>[[forget, roe, wade, law, need, made, amendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@KealanBurke Some books I love so much I keep...</td>\n",
       "      <td>[[kealanburke, book, love, much, keep, forever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>[If anyone wants to try it for themselves, her...</td>\n",
       "      <td>[[tried, hand, traditional, venezuelan, staple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>[China‘s Uncontested Candidate ‘Wins‘ Hong Kon...</td>\n",
       "      <td>[[reminder, abandoned, american, citizen, fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>[RT @JRMajewski: How is he supposed to fight i...</td>\n",
       "      <td>[[april, tucson, sector, border, patrol, agent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>[@kylejluebke Why haven’t you commented on the...</td>\n",
       "      <td>[[starmedcare, actually, two, quick, question,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>[RT @JenniferShutt: Biden says he's \"prepared ...</td>\n",
       "      <td>[[north, carolina, recovery, office, awarded, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3658 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_history  \\\n",
       "0     [\"Legislative genius\" Nancy @TeamPelosi needs ...   \n",
       "1     [withoutrunes: wack-ashimself: withoutrunes: w...   \n",
       "2     [Was just sent this after complaining about de...   \n",
       "3     [Over turn Roe V Wade fine but be prepared to ...   \n",
       "4     [@KealanBurke Some books I love so much I keep...   \n",
       "...                                                 ...   \n",
       "3655  [If anyone wants to try it for themselves, her...   \n",
       "3656  [China‘s Uncontested Candidate ‘Wins‘ Hong Kon...   \n",
       "3657  [RT @JRMajewski: How is he supposed to fight i...   \n",
       "3658  [@kylejluebke Why haven’t you commented on the...   \n",
       "3659  [RT @JenniferShutt: Biden says he's \"prepared ...   \n",
       "\n",
       "                                         cleaned_tweets  \n",
       "0     [[imagine, female, journos, stay, twitter, mus...  \n",
       "1     [[withoutrunes, wack, ashimself, withoutrunes,...  \n",
       "2     [[hahahahaha, omg, first, javascript, course, ...  \n",
       "3     [[forget, roe, wade, law, need, made, amendmen...  \n",
       "4     [[kealanburke, book, love, much, keep, forever...  \n",
       "...                                                 ...  \n",
       "3655  [[tried, hand, traditional, venezuelan, staple...  \n",
       "3656  [[reminder, abandoned, american, citizen, fami...  \n",
       "3657  [[april, tucson, sector, border, patrol, agent...  \n",
       "3658  [[starmedcare, actually, two, quick, question,...  \n",
       "3659  [[north, carolina, recovery, office, awarded, ...  \n",
       "\n",
       "[3658 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exact copy of twitter processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_preprocessing(text):\n",
    "\n",
    "    #Step 0\n",
    "    #This is Praveen's code\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+@[A-Z0-9_]+|#[A-Z0-9_]+|http\\S+\", \"\", text).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "   \n",
    "    #step 1: delete all caps words\n",
    "    t_d = re.sub(r'\\b[A-Z]+\\b', '', text)\n",
    "\n",
    "    #step 2: tokenize\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenized_doc = nltk.regexp_tokenize(t_d, pattern)\n",
    "\n",
    "    #step 3: stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stop_tokenized_doc = [word for word in tokenized_doc if word not in stopwords_list]\n",
    "\n",
    "    #step 4: lem\n",
    "    tokens = [wnl.lemmatize(word) for word in stop_tokenized_doc]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('data/unique_tweets_list.csv').drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "len(ast.literal_eval(tweet_df.iloc[750,:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    try:\n",
    "        l = ast.literal_eval(s)\n",
    "    except:\n",
    "        l = None\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tweet_history'] = tweet_df['tweet_history'].apply(lambda x: string_to_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweet_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweets(tweet_row):\n",
    "    tweet_list = []\n",
    "    for n in range(len(tweet_row)):\n",
    "        tweet_list.append(twitter_preprocessing(tweet_row[n]))\n",
    "\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweet_list:\n",
    "        if len(tweet) > 20:\n",
    "            cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['cleaned_tweets'] = tweet_df['tweet_history'].apply(lambda x: prepare_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['vectorized'] = tweet_df['cleaned_tweets'].apply(lambda x: vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-65187023354b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "\n",
    "model.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3658 entries, 0 to 3659\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   tweet_history   3658 non-null   object\n",
      " 1   cleaned_tweets  3658 non-null   object\n",
      " 2   vectorized      3658 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 114.3+ KB\n"
     ]
    }
   ],
   "source": [
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the NMF topic generation model\n",
    "\n",
    "# to get H\n",
    "H_list = []\n",
    "\n",
    "for n in range(len(tweet_df)):\n",
    "    try:\n",
    "        H_list.append(model.transform(tweet_df['vectorized'][n])) # transform document into topic vector representation\n",
    "    except:\n",
    "        continue;\n",
    "\n",
    "# to get W \n",
    "#W = model.components_ # word component weights for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List out the top 10 words for each topic\n",
    "topic_weights = []\n",
    "\n",
    "for n in range(0, len(tweet_df) - 2):\n",
    "    topic_weights.append(list(pd.DataFrame(H_list[n], columns = list(topic.values())).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_weights_df = pd.DataFrame(topic_weights,  columns = list(topic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>American Politics</th>\n",
       "      <th>War in Ukraine</th>\n",
       "      <th>Covid-19</th>\n",
       "      <th>School</th>\n",
       "      <th>Biden Administration</th>\n",
       "      <th>War in Afghanistan</th>\n",
       "      <th>Trump</th>\n",
       "      <th>Abortion</th>\n",
       "      <th>Kyle Rittenhouse Shooting</th>\n",
       "      <th>Covid-19 Vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.680626</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.731577</td>\n",
       "      <td>0.341945</td>\n",
       "      <td>0.240299</td>\n",
       "      <td>0.988894</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.254866</td>\n",
       "      <td>0.224953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.923170</td>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.087826</td>\n",
       "      <td>0.217727</td>\n",
       "      <td>0.209149</td>\n",
       "      <td>0.239076</td>\n",
       "      <td>0.324541</td>\n",
       "      <td>1.091020</td>\n",
       "      <td>0.531066</td>\n",
       "      <td>0.424512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.931812</td>\n",
       "      <td>0.098028</td>\n",
       "      <td>0.093008</td>\n",
       "      <td>0.240249</td>\n",
       "      <td>0.092589</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.268368</td>\n",
       "      <td>1.123769</td>\n",
       "      <td>0.165017</td>\n",
       "      <td>0.161810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070784</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>0.108038</td>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.110939</td>\n",
       "      <td>0.272246</td>\n",
       "      <td>0.104751</td>\n",
       "      <td>0.063113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.416697</td>\n",
       "      <td>0.222473</td>\n",
       "      <td>0.085506</td>\n",
       "      <td>0.197879</td>\n",
       "      <td>0.080960</td>\n",
       "      <td>0.106119</td>\n",
       "      <td>0.242276</td>\n",
       "      <td>1.331165</td>\n",
       "      <td>0.254181</td>\n",
       "      <td>0.133890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>1.005561</td>\n",
       "      <td>0.230707</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.395115</td>\n",
       "      <td>0.240086</td>\n",
       "      <td>0.243627</td>\n",
       "      <td>0.294857</td>\n",
       "      <td>0.704700</td>\n",
       "      <td>0.286908</td>\n",
       "      <td>0.194316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>0.609854</td>\n",
       "      <td>1.204737</td>\n",
       "      <td>0.167427</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.275848</td>\n",
       "      <td>0.073650</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.635599</td>\n",
       "      <td>0.435957</td>\n",
       "      <td>0.143159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>0.536499</td>\n",
       "      <td>0.041255</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.181352</td>\n",
       "      <td>0.072991</td>\n",
       "      <td>0.113244</td>\n",
       "      <td>0.177772</td>\n",
       "      <td>0.430502</td>\n",
       "      <td>0.161185</td>\n",
       "      <td>0.143768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>0.231382</td>\n",
       "      <td>0.393095</td>\n",
       "      <td>0.195363</td>\n",
       "      <td>0.235601</td>\n",
       "      <td>0.249610</td>\n",
       "      <td>0.250492</td>\n",
       "      <td>0.399096</td>\n",
       "      <td>0.752527</td>\n",
       "      <td>0.214983</td>\n",
       "      <td>0.289047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>0.426505</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.076170</td>\n",
       "      <td>0.148709</td>\n",
       "      <td>1.024309</td>\n",
       "      <td>0.173340</td>\n",
       "      <td>0.265617</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.561692</td>\n",
       "      <td>0.455045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3656 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      American Politics  War in Ukraine  Covid-19    School  \\\n",
       "0              0.680626        0.244300  0.119168  0.731577   \n",
       "1              0.923170        0.111925  0.087826  0.217727   \n",
       "2              0.931812        0.098028  0.093008  0.240249   \n",
       "3              0.070784        0.041267  0.005452  0.044828   \n",
       "4              0.416697        0.222473  0.085506  0.197879   \n",
       "...                 ...             ...       ...       ...   \n",
       "3651           1.005561        0.230707  0.144323  0.395115   \n",
       "3652           0.609854        1.204737  0.167427  0.091959   \n",
       "3653           0.536499        0.041255  0.045280  0.181352   \n",
       "3654           0.231382        0.393095  0.195363  0.235601   \n",
       "3655           0.426505        0.290036  0.076170  0.148709   \n",
       "\n",
       "      Biden Administration  War in Afghanistan     Trump  Abortion  \\\n",
       "0                 0.341945            0.240299  0.988894  0.486750   \n",
       "1                 0.209149            0.239076  0.324541  1.091020   \n",
       "2                 0.092589            0.135742  0.268368  1.123769   \n",
       "3                 0.108038            0.047018  0.110939  0.272246   \n",
       "4                 0.080960            0.106119  0.242276  1.331165   \n",
       "...                    ...                 ...       ...       ...   \n",
       "3651              0.240086            0.243627  0.294857  0.704700   \n",
       "3652              0.275848            0.073650  0.365628  0.635599   \n",
       "3653              0.072991            0.113244  0.177772  0.430502   \n",
       "3654              0.249610            0.250492  0.399096  0.752527   \n",
       "3655              1.024309            0.173340  0.265617  0.271836   \n",
       "\n",
       "      Kyle Rittenhouse Shooting  Covid-19 Vaccine  \n",
       "0                      0.254866          0.224953  \n",
       "1                      0.531066          0.424512  \n",
       "2                      0.165017          0.161810  \n",
       "3                      0.104751          0.063113  \n",
       "4                      0.254181          0.133890  \n",
       "...                         ...               ...  \n",
       "3651                   0.286908          0.194316  \n",
       "3652                   0.435957          0.143159  \n",
       "3653                   0.161185          0.143768  \n",
       "3654                   0.214983          0.289047  \n",
       "3655                   0.561692          0.455045  \n",
       "\n",
       "[3656 rows x 10 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(relative_weights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tucker_not_tucker_maybe_we_will_see_soon_enough = pd.Series(kmeans.predict(relative_weights_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "yay_we_did_a_model_df = pd.concat([relative_weights_df, tucker_not_tucker_maybe_we_will_see_soon_enough], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369    1\n",
       "289    1\n",
       "282    1\n",
       "281    1\n",
       "280    1\n",
       "      ..\n",
       "594    0\n",
       "597    0\n",
       "600    0\n",
       "178    0\n",
       "739    0\n",
       "Name: 0, Length: 740, dtype: int32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yay_we_did_a_model_df.iloc[:740,:][0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05540121, 0.00428809, 0.        , 0.03378609, 0.01889137,\n",
       "       0.        , 0.00330919, 0.10101941, 0.08129136, 0.        ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate tucker carlson distribution\n",
    "model.transform(td_idf_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [legislative genius nancy need go away take di...\n",
       "1       [withoutrunes wack ashimself withoutrunes wack...\n",
       "2       [sent complaining dealing nan omg i'm dying, h...\n",
       "3       [turn roe v wade fine prepared care child feed...\n",
       "4       [book love much keep forever including autogra...\n",
       "                              ...                        \n",
       "3655    [anyone want try recipe thing beyond recipe ad...\n",
       "3656    [china uncontested candidate win hong kong chi...\n",
       "3657    [rt supposed fight inflation he's busy saying ...\n",
       "3658    [commented focus conservative overturn roe v w...\n",
       "3659    [rt biden say he's prepared accept forthcoming...\n",
       "Name: cleaned_tweets, Length: 3658, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['cleaned_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec.load(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c1f5ce8eed932a4317a88fdfc83317a84584de98614c992901b7b196b5e3487"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
