{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Notebook\n",
    "This notebook will preprocess and leverage NLP models on the unstructured data to turn it into a usable feature space for modeling Tucker Carlson's body of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports cell\n",
    "\n",
    "#Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tucker document data either as a CSV or a pickle\n",
    "\n",
    "#Read out from CSV\n",
    "tucker_docs = pd.read_csv('data/tucker_docs.csv', encoding='UTF8', header = None).T\n",
    "\n",
    "#Read from pickle\n",
    "#tucker_docs = pd.read_pickle('data/tucker_pickle')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test for fun\n",
    "tucker_docs.iloc[762,0][:416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fox News host gives his take on pro-abortion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fox News host reflects on the left's respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fox News host gives his take on how Americans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fox News host gives his take on the Supreme C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fox News host gives his take on the real moti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0   Fox News host gives his take on pro-abortion ...\n",
       "1    Fox News host reflects on the left's respons...\n",
       "2   Fox News host gives his take on how Americans...\n",
       "3   Fox News host gives his take on the Supreme C...\n",
       "4   Fox News host gives his take on the real moti..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucker_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Bradley-Haderthauer Test\n",
    "Compare two topic distributions: IF BH-score is < 0.2, then a Twitterer can be confidently classified as a Tuckerbot. This person is a lower life form and unable to contribute, in good faith, to the deep state media platform of choice, Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make custom stops words to remove first 100 words? remove intro to episode \n",
    "#remove words in all caps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove words in all caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox',\n",
       " 'news',\n",
       " 'host',\n",
       " 'give',\n",
       " 'take',\n",
       " 'pro',\n",
       " 'abortion',\n",
       " 'protester',\n",
       " 'targeting',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'possible',\n",
       " 'overturn',\n",
       " 'roe',\n",
       " 'v',\n",
       " 'wade',\n",
       " 'tucker',\n",
       " 'carlson',\n",
       " 'tonight',\n",
       " 'pretty',\n",
       " 'hard',\n",
       " 'argue',\n",
       " 'people',\n",
       " 'passive',\n",
       " 'aggressive',\n",
       " 'may',\n",
       " 'tried',\n",
       " 'angry',\n",
       " 'scream',\n",
       " 'stop',\n",
       " 'violent',\n",
       " 'snarl',\n",
       " 'punch',\n",
       " 'face',\n",
       " 'passive',\n",
       " 'aggressive',\n",
       " 'people',\n",
       " 'intent',\n",
       " 'dominating',\n",
       " \"they're\",\n",
       " 'dishonest',\n",
       " 'admit',\n",
       " 'honorable',\n",
       " 'style',\n",
       " 'attack',\n",
       " 'effective',\n",
       " 'mostly',\n",
       " 'bewildering',\n",
       " 'democratic',\n",
       " 'party',\n",
       " 'practice',\n",
       " 'democrat',\n",
       " 'never',\n",
       " 'meet',\n",
       " 'open',\n",
       " 'field',\n",
       " 'battle',\n",
       " 'instead',\n",
       " 'sneak',\n",
       " 'behind',\n",
       " 'knock',\n",
       " 'unconscious',\n",
       " 'bag',\n",
       " 'sanctimony',\n",
       " 'party',\n",
       " 'weak',\n",
       " 'men',\n",
       " 'angry',\n",
       " 'woman',\n",
       " 'passive',\n",
       " 'aggression',\n",
       " 'mode',\n",
       " 'communication',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'one',\n",
       " 'jen',\n",
       " \"psaki's\",\n",
       " 'press',\n",
       " 'conference',\n",
       " 'know',\n",
       " 'exactly',\n",
       " \"we're\",\n",
       " 'talking',\n",
       " 'watched',\n",
       " 'one',\n",
       " 'yesterday',\n",
       " 'fact',\n",
       " 'last',\n",
       " 'peter',\n",
       " 'doocy',\n",
       " 'asked',\n",
       " 'psaki',\n",
       " 'administration',\n",
       " 'think',\n",
       " 'fact',\n",
       " 'liberal',\n",
       " 'group',\n",
       " 'posted',\n",
       " 'home',\n",
       " 'address',\n",
       " 'conservative',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'internet',\n",
       " 'well',\n",
       " 'obvious',\n",
       " \"they're\",\n",
       " 'one',\n",
       " 'denies',\n",
       " \"they're\",\n",
       " \"they're\",\n",
       " 'order',\n",
       " 'frighten',\n",
       " 'justice',\n",
       " 'changing',\n",
       " 'vote',\n",
       " 'roe',\n",
       " 'v',\n",
       " 'wade',\n",
       " \"that's\",\n",
       " 'illegal',\n",
       " 'clearly',\n",
       " 'federal',\n",
       " 'crime',\n",
       " 'also',\n",
       " 'way',\n",
       " 'road',\n",
       " 'chaos',\n",
       " 'collapse',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'samuel',\n",
       " 'alito',\n",
       " 'address',\n",
       " 'audience',\n",
       " 'emergency',\n",
       " 'docket',\n",
       " 'lecture',\n",
       " 'thursday',\n",
       " 'sept',\n",
       " 'mccartan',\n",
       " 'courtroom',\n",
       " 'university',\n",
       " 'notre',\n",
       " 'dame',\n",
       " 'law',\n",
       " 'school',\n",
       " 'south',\n",
       " 'bend',\n",
       " 'ind',\n",
       " 'michael',\n",
       " 'caterina',\n",
       " 'south',\n",
       " 'bend',\n",
       " 'tribune',\n",
       " 'via',\n",
       " \"can't\",\n",
       " 'allow',\n",
       " 'angry',\n",
       " 'mob',\n",
       " 'make',\n",
       " \"country's\",\n",
       " 'law',\n",
       " \"that's\",\n",
       " 'banned',\n",
       " 'lynching',\n",
       " 'jen',\n",
       " 'psaki',\n",
       " 'problem',\n",
       " 'people',\n",
       " 'understandably',\n",
       " 'upset',\n",
       " 'sam',\n",
       " \"alito's\",\n",
       " 'view',\n",
       " 'said',\n",
       " 'surprised',\n",
       " 'want',\n",
       " 'express',\n",
       " 'concern',\n",
       " 'person',\n",
       " 'screaming',\n",
       " 'sam',\n",
       " 'alito',\n",
       " 'family',\n",
       " 'official',\n",
       " 'government',\n",
       " 'position',\n",
       " 'people',\n",
       " 'protest',\n",
       " 'really',\n",
       " 'jen',\n",
       " 'psaki',\n",
       " 'rule',\n",
       " 'would',\n",
       " 'feel',\n",
       " 'angry',\n",
       " 'protester',\n",
       " 'showed',\n",
       " 'outside',\n",
       " 'say',\n",
       " 'michelle',\n",
       " \"obama's\",\n",
       " 'house',\n",
       " 'course',\n",
       " \"they'd\",\n",
       " 'thrown',\n",
       " 'thumbscrew',\n",
       " 'within',\n",
       " 'minute',\n",
       " 'charged',\n",
       " 'racism',\n",
       " 'would',\n",
       " 'applaud',\n",
       " 'languished',\n",
       " 'jail',\n",
       " 'conservative',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " \"that's\",\n",
       " 'different',\n",
       " 'story',\n",
       " 'conservative',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'deserve',\n",
       " \"that's\",\n",
       " 'jen',\n",
       " \"psaki's\",\n",
       " 'position',\n",
       " 'fact',\n",
       " 'since',\n",
       " \"we've\",\n",
       " 'speaking',\n",
       " 'topic',\n",
       " 'official',\n",
       " 'position',\n",
       " 'government',\n",
       " 'according',\n",
       " 'code',\n",
       " 'official',\n",
       " 'position',\n",
       " 'government',\n",
       " 'allowed',\n",
       " 'intimidate',\n",
       " 'judge',\n",
       " 'period',\n",
       " 'whoever',\n",
       " 'intent',\n",
       " 'interfering',\n",
       " 'obstructing',\n",
       " 'impeding',\n",
       " 'administration',\n",
       " 'justice',\n",
       " 'intent',\n",
       " 'influencing',\n",
       " 'judge',\n",
       " 'juror',\n",
       " 'witness',\n",
       " 'court',\n",
       " 'officer',\n",
       " 'discharge',\n",
       " 'duty',\n",
       " 'picket',\n",
       " 'parade',\n",
       " 'near',\n",
       " 'building',\n",
       " 'housing',\n",
       " 'court',\n",
       " 'united',\n",
       " 'state',\n",
       " 'near',\n",
       " 'building',\n",
       " 'residence',\n",
       " 'occupied',\n",
       " 'used',\n",
       " 'judge',\n",
       " 'juror',\n",
       " 'witness',\n",
       " 'court',\n",
       " 'officer',\n",
       " 'intent',\n",
       " 'us',\n",
       " 'sound',\n",
       " 'truck',\n",
       " 'similar',\n",
       " 'device',\n",
       " 'resort',\n",
       " 'demonstration',\n",
       " 'near',\n",
       " 'building',\n",
       " 'residence',\n",
       " 'shall',\n",
       " 'fined',\n",
       " 'imprisoned',\n",
       " 'clearly',\n",
       " 'crime',\n",
       " 'jen',\n",
       " 'psaki',\n",
       " 'either',\n",
       " 'know',\n",
       " 'far',\n",
       " 'likely',\n",
       " 'care',\n",
       " 'far',\n",
       " 'biden',\n",
       " 'administration',\n",
       " 'concerned',\n",
       " 'justice',\n",
       " 'alito',\n",
       " 'responsible',\n",
       " 'threat',\n",
       " 'family',\n",
       " 'want',\n",
       " 'targeted',\n",
       " 'mob',\n",
       " 'write',\n",
       " 'opinion',\n",
       " 'mob',\n",
       " 'like',\n",
       " 'dumbo',\n",
       " 'fault',\n",
       " \"we're\",\n",
       " 'hurting',\n",
       " \"that's\",\n",
       " 'always',\n",
       " 'position',\n",
       " 'passive',\n",
       " 'aggressive',\n",
       " 'people',\n",
       " 'position',\n",
       " 'passive',\n",
       " 'aggressive',\n",
       " 'party',\n",
       " 'recognized',\n",
       " 'immediately',\n",
       " 'watch',\n",
       " 'stuff',\n",
       " 'living',\n",
       " 'saw',\n",
       " 'made',\n",
       " 'u',\n",
       " 'think',\n",
       " 'ran',\n",
       " 'morning',\n",
       " 'day',\n",
       " 'assumed',\n",
       " 'based',\n",
       " 'quite',\n",
       " 'bit',\n",
       " 'evidence',\n",
       " 'main',\n",
       " 'threat',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'right',\n",
       " 'justice',\n",
       " 'system',\n",
       " 'come',\n",
       " 'group',\n",
       " 'activist',\n",
       " 'liberal',\n",
       " 'enraged',\n",
       " 'idea',\n",
       " 'roe',\n",
       " 'v',\n",
       " 'wade',\n",
       " 'might',\n",
       " 'repealed',\n",
       " 'people',\n",
       " 'spray',\n",
       " 'painting',\n",
       " 'church',\n",
       " 'putting',\n",
       " 'sam',\n",
       " \"alito's\",\n",
       " 'home',\n",
       " 'address',\n",
       " 'internet',\n",
       " \"they're\",\n",
       " 'mob',\n",
       " \"they're\",\n",
       " 'angry',\n",
       " 'know',\n",
       " \"they're\",\n",
       " 'angry',\n",
       " 'whole',\n",
       " 'thing',\n",
       " 'made',\n",
       " 'sense',\n",
       " 'u',\n",
       " 'according',\n",
       " \"that's\",\n",
       " \"what's\",\n",
       " 'happening',\n",
       " 'fact',\n",
       " 'opposite',\n",
       " \"what's\",\n",
       " 'happening',\n",
       " 'real',\n",
       " 'threat',\n",
       " 'informed',\n",
       " 'u',\n",
       " 'morning',\n",
       " 'come',\n",
       " 'people',\n",
       " 'angry',\n",
       " 'sam',\n",
       " \"alito's\",\n",
       " 'opinion',\n",
       " 'people',\n",
       " 'happy',\n",
       " 'one',\n",
       " 'celebrating',\n",
       " 'sam',\n",
       " 'alito',\n",
       " 'wrote',\n",
       " 'one',\n",
       " 'real',\n",
       " 'danger',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'watch',\n",
       " 'morning',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'official',\n",
       " 'preparing',\n",
       " 'potential',\n",
       " 'violence',\n",
       " 'capitol',\n",
       " 'nationwide',\n",
       " 'leak',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'draft',\n",
       " 'opinion',\n",
       " 'would',\n",
       " 'strike',\n",
       " 'roe',\n",
       " 'v',\n",
       " 'wade',\n",
       " 'year',\n",
       " 'capitol',\n",
       " 'police',\n",
       " 'warning',\n",
       " 'far',\n",
       " 'right',\n",
       " 'calling',\n",
       " 'violence',\n",
       " 'religious',\n",
       " 'group',\n",
       " 'planning',\n",
       " 'rally',\n",
       " 'abortion',\n",
       " 'right',\n",
       " 'whitney',\n",
       " 'wild',\n",
       " 'live',\n",
       " 'outside',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'listen',\n",
       " 'seeing',\n",
       " 'fence',\n",
       " 'go',\n",
       " 'sort',\n",
       " 'like',\n",
       " 'post',\n",
       " 'january',\n",
       " 'seeing',\n",
       " 'specific',\n",
       " 'intelligence',\n",
       " 'worrying',\n",
       " 'police',\n",
       " 'official',\n",
       " 'well',\n",
       " 'point',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'source',\n",
       " 'several',\n",
       " 'tell',\n",
       " 'u',\n",
       " 'closely',\n",
       " 'monitoring',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'chatter',\n",
       " 'suggests',\n",
       " \"there's\",\n",
       " 'potential',\n",
       " 'violence',\n",
       " 'abortion',\n",
       " 'clinic',\n",
       " 'provider',\n",
       " 'abortion',\n",
       " 'clinic',\n",
       " 'staff',\n",
       " 'member',\n",
       " 'judiciary',\n",
       " 'would',\n",
       " 'include',\n",
       " 'justice',\n",
       " 'well',\n",
       " 'member',\n",
       " 'federal',\n",
       " 'government',\n",
       " 'know',\n",
       " 'january',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'know',\n",
       " 'across',\n",
       " 'country',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'chatter',\n",
       " 'manifest',\n",
       " 'actual',\n",
       " 'violence',\n",
       " 'sometimes',\n",
       " 'people',\n",
       " 'really',\n",
       " 'say',\n",
       " \"they're\",\n",
       " 'going',\n",
       " 'got',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'source',\n",
       " 'tell',\n",
       " 'real',\n",
       " 'threat',\n",
       " 'far',\n",
       " 'right',\n",
       " 'far',\n",
       " 'right',\n",
       " 'fence',\n",
       " 'going',\n",
       " 'outside',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'far',\n",
       " 'right',\n",
       " 'pose',\n",
       " 'threat',\n",
       " 'far',\n",
       " 'right',\n",
       " 'one',\n",
       " 'watch',\n",
       " \"they're\",\n",
       " 'one',\n",
       " 'whose',\n",
       " 'electronic',\n",
       " 'communication',\n",
       " 'got',\n",
       " 'monitor',\n",
       " 'closely',\n",
       " 'make',\n",
       " 'sense',\n",
       " 'roe',\n",
       " 'v',\n",
       " 'wade',\n",
       " 'may',\n",
       " 'going',\n",
       " 'away',\n",
       " 'people',\n",
       " 'going',\n",
       " 'want',\n",
       " 'bomb',\n",
       " 'many',\n",
       " 'abortion',\n",
       " 'clinic',\n",
       " 'hurt',\n",
       " 'many',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'justice',\n",
       " 'possible',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'may',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'law',\n",
       " 'hated',\n",
       " 'year',\n",
       " 'see',\n",
       " 'would',\n",
       " 'moved',\n",
       " 'violence',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'first',\n",
       " 'response',\n",
       " 'laugh',\n",
       " 'seems',\n",
       " 'absurd',\n",
       " 'real',\n",
       " 'dumb',\n",
       " 'think',\n",
       " 'remembered',\n",
       " 'president',\n",
       " 'united',\n",
       " 'state',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'telling',\n",
       " 'u',\n",
       " 'year',\n",
       " \"they've\",\n",
       " 'telling',\n",
       " 'u',\n",
       " 'matter',\n",
       " 'happens',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'look',\n",
       " 'like',\n",
       " 'people',\n",
       " 'vote',\n",
       " 'u',\n",
       " 'yeah',\n",
       " 'people',\n",
       " 'vote',\n",
       " 'u',\n",
       " 'dangerous',\n",
       " 'one',\n",
       " 'always',\n",
       " 'fault',\n",
       " 'watch',\n",
       " 'seen',\n",
       " 'dangerous',\n",
       " 'threat',\n",
       " 'democracy',\n",
       " 'invasion',\n",
       " 'capitol',\n",
       " 'greatest',\n",
       " 'terrorism',\n",
       " 'related',\n",
       " 'threat',\n",
       " 'face',\n",
       " 'homeland',\n",
       " 'threat',\n",
       " 'domestic',\n",
       " 'violent',\n",
       " 'extremism',\n",
       " 'according',\n",
       " 'intelligence',\n",
       " 'community',\n",
       " 'terrorism',\n",
       " 'white',\n",
       " 'supremacy',\n",
       " 'lethal',\n",
       " 'threat',\n",
       " 'homeland',\n",
       " 'today',\n",
       " 'al',\n",
       " 'qaeda',\n",
       " 'white',\n",
       " 'supremacist',\n",
       " 'al',\n",
       " 'qaeda',\n",
       " 'white',\n",
       " 'supremacy',\n",
       " 'reviewed',\n",
       " 'tape',\n",
       " 'started',\n",
       " 'make',\n",
       " 'sense',\n",
       " 'like',\n",
       " 'mongol',\n",
       " 'visigoth',\n",
       " 'dreaded',\n",
       " 'white',\n",
       " 'supremacist',\n",
       " 'declared',\n",
       " 'war',\n",
       " 'civilization',\n",
       " 'trump',\n",
       " 'voter',\n",
       " 'evidence',\n",
       " 'around',\n",
       " 'u',\n",
       " \"we've\",\n",
       " 'ignored',\n",
       " 'level',\n",
       " 'want',\n",
       " 'know',\n",
       " 'retrospect',\n",
       " 'always',\n",
       " 'right',\n",
       " 'front',\n",
       " 'u',\n",
       " 'recall',\n",
       " 'trump',\n",
       " 'voter',\n",
       " 'looted',\n",
       " \"macy's\",\n",
       " 'new',\n",
       " 'york',\n",
       " 'city',\n",
       " 'help',\n",
       " \"that's\",\n",
       " 'torched',\n",
       " 'police',\n",
       " 'precinct',\n",
       " 'minneapolis',\n",
       " 'symbol',\n",
       " 'law',\n",
       " 'set',\n",
       " 'fire',\n",
       " 'burned',\n",
       " 'federal',\n",
       " 'building',\n",
       " 'portland',\n",
       " 'oregon',\n",
       " 'president',\n",
       " 'joe',\n",
       " 'biden',\n",
       " 'delivers',\n",
       " 'remark',\n",
       " 'debt',\n",
       " 'ceiling',\n",
       " 'event',\n",
       " 'state',\n",
       " 'dining',\n",
       " 'room',\n",
       " 'white',\n",
       " 'house',\n",
       " 'monday',\n",
       " 'oct',\n",
       " 'washington',\n",
       " 'photo',\n",
       " 'evan',\n",
       " 'vucci',\n",
       " 'case',\n",
       " 'forgotten',\n",
       " 'right',\n",
       " 'wing',\n",
       " 'attack',\n",
       " 'look',\n",
       " 'like',\n",
       " 'white',\n",
       " 'supremacist',\n",
       " \"here's\",\n",
       " 'see',\n",
       " \"they're\",\n",
       " 'putting',\n",
       " 'barrier',\n",
       " 'front',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'people',\n",
       " 'happy',\n",
       " 'sam',\n",
       " \"alito's\",\n",
       " 'opinion',\n",
       " 'might',\n",
       " 'start',\n",
       " 'burning',\n",
       " 'looting',\n",
       " 'course',\n",
       " 'would',\n",
       " 'consistent',\n",
       " 'year',\n",
       " 'long',\n",
       " 'reign',\n",
       " 'right',\n",
       " 'wing',\n",
       " 'terror',\n",
       " \"we've\",\n",
       " 'living',\n",
       " 'thirst',\n",
       " 'anarchy',\n",
       " 'chaos',\n",
       " 'never',\n",
       " 'sated',\n",
       " 'one',\n",
       " 'point',\n",
       " 'trump',\n",
       " 'voter',\n",
       " 'seceded',\n",
       " 'united',\n",
       " 'state',\n",
       " 'set',\n",
       " 'nation',\n",
       " 'lawlessness',\n",
       " 'drug',\n",
       " 'squalor',\n",
       " 'violence',\n",
       " 'within',\n",
       " 'city',\n",
       " 'limit',\n",
       " 'seattle',\n",
       " 'like',\n",
       " 'confederate',\n",
       " 'weed',\n",
       " 'spray',\n",
       " 'paint',\n",
       " 'right',\n",
       " 'middle',\n",
       " 'seattle',\n",
       " 'talk',\n",
       " 'brazen',\n",
       " \"that's\",\n",
       " 'could',\n",
       " 'happen',\n",
       " 'town',\n",
       " 'wonder',\n",
       " \"america's\",\n",
       " 'abortionist',\n",
       " 'community',\n",
       " 'cowering',\n",
       " 'fear',\n",
       " 'tonight',\n",
       " 'get',\n",
       " 'worse',\n",
       " 'people',\n",
       " 'little',\n",
       " 'respect',\n",
       " 'law',\n",
       " 'order',\n",
       " 'think',\n",
       " 'nothing',\n",
       " 'rushing',\n",
       " 'building',\n",
       " 'department',\n",
       " 'store',\n",
       " 'walgreens',\n",
       " 'liquor',\n",
       " 'store',\n",
       " 'foot',\n",
       " 'locker',\n",
       " 'taking',\n",
       " 'want',\n",
       " 'steal',\n",
       " 'armful',\n",
       " 'merchandise',\n",
       " 'shelf',\n",
       " 'walk',\n",
       " 'door',\n",
       " 'like',\n",
       " 'purely',\n",
       " 'amuse',\n",
       " 'push',\n",
       " 'stranger',\n",
       " 'front',\n",
       " 'train',\n",
       " 'open',\n",
       " 'fire',\n",
       " 'subway',\n",
       " 'car',\n",
       " 'drive',\n",
       " 'truck',\n",
       " 'christmas',\n",
       " 'parade',\n",
       " 'beat',\n",
       " 'elderly',\n",
       " 'asian',\n",
       " 'woman',\n",
       " 'street',\n",
       " \"can't\",\n",
       " 'control',\n",
       " 'people',\n",
       " \"they're\",\n",
       " 'beyond',\n",
       " 'reach',\n",
       " 'logic',\n",
       " 'primitive',\n",
       " 'superstitious',\n",
       " 'come',\n",
       " 'believe',\n",
       " 'defeat',\n",
       " 'nature',\n",
       " 'magic',\n",
       " 'amulet',\n",
       " 'like',\n",
       " 'cosmetic',\n",
       " \"women's\",\n",
       " 'clothing',\n",
       " 'think',\n",
       " \"they're\",\n",
       " 'shapeshifters',\n",
       " 'change',\n",
       " 'physical',\n",
       " 'form',\n",
       " 'become',\n",
       " 'something',\n",
       " 'different',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'screen',\n",
       " 'right',\n",
       " 'people',\n",
       " 'like',\n",
       " 'believe',\n",
       " 'history',\n",
       " 'science',\n",
       " \"they're\",\n",
       " 'barely',\n",
       " 'literate',\n",
       " 'communicate',\n",
       " 'instead',\n",
       " 'grunt',\n",
       " 'unintelligible',\n",
       " 'sound',\n",
       " \"here's\",\n",
       " 'leader',\n",
       " 'chief',\n",
       " 'tribe',\n",
       " 'speaking',\n",
       " 'language',\n",
       " 'understand',\n",
       " 'best',\n",
       " 'way',\n",
       " 'get',\n",
       " 'something',\n",
       " 'done',\n",
       " 'hold',\n",
       " 'near',\n",
       " 'dear',\n",
       " 'like',\n",
       " 'able',\n",
       " 'anyway',\n",
       " 'say',\n",
       " 'know',\n",
       " 'right',\n",
       " 'supremacist',\n",
       " 'person',\n",
       " 'understand',\n",
       " 'esoteric',\n",
       " 'language',\n",
       " 'primitive',\n",
       " 'adorn',\n",
       " 'body',\n",
       " 'fantastic',\n",
       " 'decoration',\n",
       " 'dress',\n",
       " 'costume',\n",
       " 'even',\n",
       " 'push',\n",
       " 'piece',\n",
       " 'metal',\n",
       " 'face',\n",
       " 'order',\n",
       " 'look',\n",
       " 'fierce',\n",
       " 'look',\n",
       " 'war',\n",
       " 'paint',\n",
       " 'hi',\n",
       " \"name's\",\n",
       " 'preschool',\n",
       " 'teacher',\n",
       " 'recently',\n",
       " 'started',\n",
       " 'wearing',\n",
       " 'pronoun',\n",
       " 'pin',\n",
       " 'kid',\n",
       " 'get',\n",
       " 'pick',\n",
       " 'new',\n",
       " 'pronoun',\n",
       " 'pin',\n",
       " 'every',\n",
       " 'day',\n",
       " 'pick',\n",
       " 'every',\n",
       " 'single',\n",
       " 'day',\n",
       " 'change',\n",
       " 'non',\n",
       " 'binary',\n",
       " 'preschool',\n",
       " 'teacher',\n",
       " 'kid',\n",
       " 'know',\n",
       " 'non',\n",
       " 'binary',\n",
       " 'know',\n",
       " 'girl',\n",
       " 'boy',\n",
       " 'use',\n",
       " 'pronoun',\n",
       " 'classroom',\n",
       " 'work',\n",
       " 'kid',\n",
       " 'get',\n",
       " \"that's\",\n",
       " 'okay',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tucker_doc = tucker_docs.iloc[0,0]\n",
    "#pattern to delete words in all caps\n",
    "#pattern = \"(([a-zA-Z]+(?:'[a-z]+)?))\"\n",
    "t_d = re.sub(r'\\b[A-Z]+\\b', '', tucker_doc)\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenized_doc = nltk.regexp_tokenize(t_d, pattern)\n",
    "\n",
    "#lowercase all words\n",
    "tokenized_doc = [word.lower() for word in tokenized_doc]\n",
    "tokenized_doc\n",
    "\n",
    "#Freqdist\n",
    "td_freqdist = FreqDist(tokenized_doc)\n",
    "td_freqdist.most_common(75)\n",
    "\n",
    "#Stop words\n",
    "stopwords_list = stopwords.words('english')\n",
    "stop_tokenized_doc = [word for word in tokenized_doc if word not in stopwords_list]\n",
    "\n",
    "#Stopped freqdist \n",
    "stop_td_freqdist = FreqDist(stop_tokenized_doc)\n",
    "stop_td_freqdist.most_common(75)\n",
    "\n",
    "#lemmatize\n",
    "#to lem \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#more lemmatizin\n",
    "#wnl.lemmatize(wn1.lemmatize(word) for word in stop_tokenized_doc)\n",
    "tokens = [wnl.lemmatize(word) for word in stop_tokenized_doc]\n",
    "tokens\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#def lemmatize_words(text):\n",
    "   # return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "#stop_tokenized_doc = stop_tokenized_doc.apply(lambda text: lemmatize_words(stop_tokenized_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess function to do all steps above at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    #Step 0\n",
    "    #This is Praveen's code but it's not PEP-8 friendly so you should fix that for next cohort ty\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+@[A-Z0-9_]+|#[A-Z0-9_]+|http\\S+\", \"\", text).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "   \n",
    "    #step 1: delete all caps words\n",
    "    t_d = re.sub(r'\\b[A-Z]+\\b', '', text)\n",
    "\n",
    "    #step 2: tokenize\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    tokenized_doc = nltk.regexp_tokenize(t_d, pattern)\n",
    "\n",
    "    #step 3: stop words\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stop_tokenized_doc = [word for word in tokenized_doc if word not in stopwords_list]\n",
    "\n",
    "    #step 4: lem\n",
    "    tokens = [wnl.lemmatize(word) for word in stop_tokenized_doc]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tucker_list = tucker_docs[0].tolist()\n",
    "new_list = []\n",
    "for each_doc in tucker_list:\n",
    "    new_list.append(preprocessing(each_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9z/tw9j8p_n7w3fdl6dpkycs1p00000gn/T/ipykernel_2318/1561738870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnot_so_sparse_not_so_spicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "feature_names = new_list[0][0].get_feature_names()\n",
    "not_so_sparse_not_so_spicy = pd.DataFrame(new_list[0][1].toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'not_so_sparse_not_so_spicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9z/tw9j8p_n7w3fdl6dpkycs1p00000gn/T/ipykernel_2318/1834643827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnot_so_sparse_not_so_spicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'not_so_sparse_not_so_spicy' is not defined"
     ]
    }
   ],
   "source": [
    "not_so_sparse_not_so_spicy.sort_values(by = ['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9z/tw9j8p_n7w3fdl6dpkycs1p00000gn/T/ipykernel_2318/3373340936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1310\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "vect = vectorize.fit_transform(new_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9z/tw9j8p_n7w3fdl6dpkycs1p00000gn/T/ipykernel_2318/3987904687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vect' is not defined"
     ]
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('data/unique_tweets_list.csv').drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "len(ast.literal_eval(tweet_df.iloc[750,:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    try:\n",
    "        l = ast.literal_eval(s)\n",
    "    except:\n",
    "        l = None\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tweet_history'] = tweet_df['tweet_history'].apply(lambda x: string_to_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweet_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweets(tweet_row):\n",
    "    tweet_list = []\n",
    "    for n in range(len(tweet_row)):\n",
    "        tweet_list.append(preprocessing(tweet_row[n]))\n",
    "\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweet_list:\n",
    "        if len(tweet) > 20:\n",
    "            cleaned_tweets.append(tweet)\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['cleaned_tweets'] = tweet_df['tweet_history'].apply(lambda x: prepare_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c1f5ce8eed932a4317a88fdfc83317a84584de98614c992901b7b196b5e3487"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
